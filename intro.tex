\section{Introduction}

Internet-based ``citizen-science'' enables hundreds of thousands of untrained human volunteers to contribute to scientific research across a variety of domains \cite{citizen-science}. Reviving the old tradition of ``amateur'' scientific discovery, which includes prestigious names such as Darwin, Leibniz, and Newton, projects such as eBird \cite{eBird}, FoldIt \cite{foldit}, OldWeather \cite{oldweather}, and Stardust@Home \cite{stardust} convincingly demonstrate that a crowdsourced approach to science can be valuable both to the participants, as educational tools and cognitively-stimulating puzzles \cite{citizen-science-in-curricula}, and the broader research community. It offers a scalable and accurate means to gather and analyze large data sets, and help solving challenging problems, which, due to their size and complexity, remain difficult to master within existing scientific teams or with the support of state-of-the-art computing technology \cite{fortson-2011, lintott-08, lintott-11, simpson-12, davis-11}.

To date successful citizen science is almost impossible to replicate. The reasons for this state of affairs are not only related to the novelty of the field, but also to the challenges intrinsically associated with the creation and effective support of an online community of volunteers collaboratively pursuing research. This scenario is arguably much more complex than other forms of crowdsourcing, such as microtask platforms, for instance Amazon's Mechanical Turk\footnote{Mechanical Turk - \url{www.amazon.com/mturk}}, which operate with a very constrained set of mechanisms to accomplish tasks and interact, and an arguably better manageable incentives scheme; or challenges and open innovation, for instance InnoCentive\footnote{InnoCentive - \url{www.innocentive.com}}, which attract crowds many order of magnitudes smaller than most Zooniverse projects. In this scenario hundreds of thousands of people engage with the system in many ways beyond performing the actual tasks; they take part in discussions, share items they found interesting, interact with professional research teams, and participate in the definition of new scientific workflows. This reflects the varied, and often idiosyncratic motivations that drive their volunteer efforts, the landscape of which is still only starting to be understood \cite{raddick2010galaxy}. 

Even when such considerations have been made, citizen science systems can fail for many reasons, often cited in the literature concerned with the development of sustainable online participatory systems in various domains (TODO references). One important factor is the nature of the task to be crowdsourced. For some projects, the domain of the task may be intrinsically less interesting to members of the general public than others; the subjects being examined might be perceived as unpleasant, or even off-putting; tasks referring to, for example, the identification of dead animals, or the location of parasites in tissues of living patients may fall into this category. Such issues demand for a different set of incentives and motivators than the one which is assumed to apply in most citizen-science projects. In other cases, the task could turn out to be too complicated to be approached in a loosely controlled collaborative environment, or could be hardly solved in a distributed manner, thus involving a significant coordination overhead. Other factors that could contribute to a project failing include aspects of the ways it is announced or launched, or the communications channels and resources allocated to provide community support over time.  Merely the ways that the community is structured to support these projects, such as how moderators are chosen and the powers and responsibilities they are given, could further influence long-term participation in the project as a whole. The combination of these factors means that designers must both cope with a high-dimensional, complex design space that is difficult to explore, and outcomes that are quite unpredictable and uncertain (e.g., \cite{ebird, ubiome, druschke2012failures}).

In this paper we contribute a detailed case study of a citizen-science platform called \emph{Zooniverse} \footnote{Zooniverse - \url{www.zooniverse.org}}, which evolved from a one-time experiment in crowdsourced data analysis in astronomy to a true ``factory for citizen science'', an authority for generating successful Web applications for research teams in different domains. As of September $2013$, Zooniverse-assisted research resulted in scientific findings published in over (TODO -- $57$) journal articles, advancing the state of the art in disciplines as diverse as astrophysics, zoology, archaeology, cell and marine biology, and climatology. This was achieved through the sustained effort of a dedicated and efficacious community of more than $800.000$ of users, that pursue (and often complete) new projects and analyze large collections of images, video, and audio records soon after their release. Perhaps of most significance, several Zooniverse projects have benefited from \emph{citizen-initiated} serendipitous scientific discoveries, which have resulted in bi-directional collaborative processes between the volunteers and professional scientists, for evidence collection, hypothesis generation, and validation.

Our case study will shed light on those aspects in the design and operation of citizen-science projects that have played a determinant role in the incredible success story behind Zooniverse. It will build upon the vast know-how of the Zooniverse team to define a set of guidelines and best practices in this emerging new field, which will inform the development of new systems that help resolve important research questions and sustain participation over time. As of today the Zooniverse team has been involved in $27$ citizen-science projects in natural sciences and the humanities, covering a wide variety of data sets and data analysis tasks. This project portfolio gives them a unique perspective on the many variations of needs pertaining to crowdsourced science, and extensive experience in designing and deploying such systems in production. The engineering of these systems followed an evolutionary, iterative approach, applying insights derived from each project to the next, with some projects being re-designed and re-launched several times. This process permitted the Zooniverse team to study design decisions at multiple levels, from the definition of the tasks the contributors are expected to undertake, to the tools by which members of the community interact with each other and with the science teams coordinating the projects, the user interfaces, and the strategies implemented when launching a new project or encouraging prolonged user engagement. 

The case study summarizes the findings of a retrospective reflective thematic analysis conducted with core members of the Zooniverse team, to identify the key design decisions that were made, and to document the informal knowledge gained across the various projects they have successfully launched and managed. Our aim was to allow perspectives on the complicated design space in which citizen-science projects come into existence to be shared with both UX practitioners interested in designing such systems, as well as to contextualise the underlying design dimensions against the growing body of crowdsourcing and online community studies being conducted in the HCI research community.

We begin with a short history of the project in order to introduce readers to the broader context for the case study, followed by a dimensional design analysis of particular aspects of the design and deployment of Zooniverse projects.  Finally, we discuss the perspective of the Zooniverse team regarding the greatest difficulties they faced in building effective citizen science: $X$, $Y$ and $Z$, and ways that HCI research may be able to help.


